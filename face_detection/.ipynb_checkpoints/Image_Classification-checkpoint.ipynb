{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precog Project\n",
    "## Image Recognition Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  I have a folder in which named clean_dataset, which has 3 sub folder named namo, ak and people \n",
    "#  containing 50 images in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Basic Idea about problem is that is looks like typical Classification Problem, \n",
    "# Where classes can be namely NAMO, AK, People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "# import dlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now want to convert the entire dataset into a 3D array\n",
    "# (image index, x, y), with mean = 0  and s.d. = 0.5\n",
    "#  give 3 diff folderpaths namo, ak, people\n",
    "img_wd = 200 # resized Images\n",
    "img_ht = 200 # resized Images\n",
    "\n",
    "def load_pictures(folderpath):\n",
    "    '''Loading for a single image'''\n",
    "    img_files = os.listdir(folderpath)\n",
    "    tot_imgs = len(img_files)\n",
    "\n",
    "#     creating dataset for images of each class, dataset will be of size [totalimages * (img_wd*img_ht*3 =(600))]\n",
    "    dataset = np.ndarray(shape = (tot_imgs, img_wd*img_ht*3),\n",
    "                         dtype = np.float32)\n",
    "    img_index = 0\n",
    "    for img in img_files:\n",
    "        img_file_path = os.path.join(folderpath, img)\n",
    "        try:\n",
    "            img_data = (ndimage.imread(img_file_path).astype(float) - 255.0/2)/255.0\n",
    "#              We did the above to set the range of the image data to \n",
    "#             -0.5 to 0.5 (s.d.)\n",
    "\n",
    "#             reshaping a (200,200,3) matrix to (12000), flattening\n",
    "            img_data = img_data.reshape(img_wd*img_ht*3)\n",
    "    \n",
    "#     adding flattened img_data to dataset\n",
    "            dataset[img_index, :] = img_data\n",
    "            img_index = img_index + 1\n",
    "        except IOError as e:\n",
    "            print('file read error, skipping thsi file')\n",
    "                \n",
    "    print('Full dataset shape', dataset.shape)\n",
    "    print('Dataset Mean', np.mean(dataset))\n",
    "    print('Dataset SD', np.std(dataset))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharma/.local/lib/python2.7/site-packages/ipykernel_launcher.py:28: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', -0.065289788)\n",
      "('Dataset SD', 0.2462582)\n",
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', 0.0063913679)\n",
      "('Dataset SD', 0.22900349)\n",
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', -0.030012263)\n",
      "('Dataset SD', 0.26703113)\n"
     ]
    }
   ],
   "source": [
    "data_set_ak = load_pictures(\"./clean_dataset/ak\")\n",
    "data_set_namo = load_pictures(\"./clean_dataset/namo\")\n",
    "data_set_people = load_pictures(\"./clean_dataset/people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now i have created Flattened Image data set for each three \n",
    "# of categories\n",
    "\n",
    "#  one such db can be \n",
    "# [ R,G,B,R,G,B,R,G,B,R,G,B,R,G,B,R,G,B ..... 200*200  ] pixals\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# for all images for a given person\n",
    "# so now size of a given db is (400*20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now I have to divide dataset in TRAIN, TEST, CROSS_VALIDATION\n",
    "# total 400 images of each\n",
    "# 50% --> TRAIN (300 imgs in each class)\n",
    "# 25% -->TEST (50 imgs '' )\n",
    "# 25% -->CV (50 imgs '')\n",
    "# CV = Cross Validation Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_train_namo = data_set_namo[0:300]\n",
    "dataset_test_namo = data_set_namo[300:350]\n",
    "dataset_cv_namo = data_set_namo[350:400]\n",
    "\n",
    "dataset_train_ak = data_set_ak[0:300]\n",
    "dataset_test_ak = data_set_ak[300:350]\n",
    "dataset_cv_ak = data_set_ak[350:400]\n",
    "\n",
    "dataset_train_people = data_set_people[0:300]\n",
    "dataset_test_people = data_set_people[300:350]\n",
    "dataset_cv_people = data_set_people[350:400]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 120000)\n",
      "(150, 120000)\n",
      "(150, 120000)\n"
     ]
    }
   ],
   "source": [
    "dataset_train_final = np.concatenate((dataset_train_namo,dataset_train_ak))\n",
    "dataset_train_final = np.concatenate((dataset_train_final,dataset_train_people))\n",
    "print dataset_train_final.shape\n",
    "\n",
    "dataset_test_final = np.concatenate((dataset_test_namo,dataset_test_ak))\n",
    "dataset_test_final = np.concatenate((dataset_test_final,dataset_test_people))\n",
    "print dataset_test_final.shape\n",
    "\n",
    "dataset_cv_final = np.concatenate((dataset_cv_namo,dataset_cv_ak))\n",
    "dataset_cv_final = np.concatenate((dataset_cv_final,dataset_cv_people))\n",
    "print dataset_test_final.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  next task to create a label for each dataset \n",
    "# if 1 represents modi\n",
    "# it should be like\n",
    "# [1]\n",
    "# [1]\n",
    "# [1]\n",
    "# [1]\n",
    "\n",
    "# CONVENTION -\n",
    "# 1 --> NAMO\n",
    "# 2 --> AK\n",
    "# 3 --> PEOPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# total row in train data set = 900\n",
    "\n",
    "label_train = np.ndarray(900, dtype=np.int32)\n",
    "label_train[0:300]=1 # for namo \n",
    "label_train[300:600]=2 # for ak\n",
    "label_train[600:900]=3 # for people\n",
    "print label_train.size\n",
    "\n",
    "label_test = np.ndarray(150, dtype=np.int32)\n",
    "label_test[0:50]=1 # for namo \n",
    "label_test[50:100]=2 # for ak\n",
    "label_test[100:150]=3 # for people\n",
    "print label_test.size\n",
    "\n",
    "\n",
    "label_cv = np.ndarray(150, dtype=np.int32)\n",
    "label_cv[0:50]=1 # for namo \n",
    "label_cv[50:100]=2 # for ak\n",
    "label_cv[100:150]=3 # for people\n",
    "print label_cv.size\n",
    "\n",
    "#     just an other way to label, I was trying\n",
    "# label_array_namo = np.full((400,1),1)\n",
    "# label_array_ak = np.full((400,1),2)\n",
    "# label_array_people = np.full((400,1),3)\n",
    "# # print label_array_ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# both dataset and labels are arranged w.r.t. variable perm \n",
    "\n",
    "def randomize(dataset,labels,size):\n",
    "    perm = np.random.permutation(size)\n",
    "    shuffled_dataset = dataset[perm,:]\n",
    "    shuffled_labels = labels[perm]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 120000) (900,)\n",
      "(150, 120000) (150,)\n",
      "(150, 120000) (150,)\n",
      "[1 3 3 3 3 3 2 2 1 2 1 1 3 3 1 3 3 2 3 2 3 3 3 3 2 2 3 1 1 2 2 1 1 1 2 2 1\n",
      " 1 2 1 2 2 1 1 2 2 1 2 3 2 3 3 1 3 2 1 1 3 1 3 3 2 2 1 3 2 3 2 1 3 1 3 3 2\n",
      " 2 1 1 2 3 3 1 3 2 2 1 2 1 1 2 2 3 1 1 3 2 1 2 1 2 3 1 1 2 2 3 3 3 3 1 1 3\n",
      " 2 1 2 2 2 1 2 1 3 2 3 3 1 2 1 3 1 1 1 3 3 2 1 2 2 3 1 3 3 2 1 3 3 2 2 1 1\n",
      " 2 3]\n"
     ]
    }
   ],
   "source": [
    "dataset_train_final, label_train = randomize(dataset_train_final,label_train,900)\n",
    "dataset_test_final, label_test = randomize(dataset_test_final,label_test,150)\n",
    "dataset_cv_final, label_cv = randomize(dataset_cv_final,label_cv,150)\n",
    "\n",
    "print dataset_train_final.shape,label_train.shape\n",
    "print dataset_test_final.shape, label_test.shape\n",
    "print dataset_cv_final.shape, label_cv.shape\n",
    "# A sample printout, to be sure of ramdomised data\n",
    "print label_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving files to .pickle\n",
    "\n",
    "with open(\"./dataset_train_final.pickle\",'wb') as f:\n",
    "    pickle.dump(dataset_train_final,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./label_train.pickle\",'wb') as f:\n",
    "    pickle.dump(label_train,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./dataset_test_final.pickle\",'wb') as f:\n",
    "    pickle.dump(dataset_test_final,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./label_test.pickle\",'wb') as f:\n",
    "    pickle.dump(label_test,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./dataset_cv_final.pickle\",'wb') as f:\n",
    "    pickle.dump(dataset_cv_final,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./label_cv.pickle\",'wb') as f:\n",
    "    pickle.dump(label_cv,f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  for fetching data from .pickle files\n",
    "\n",
    "with open(\"./dataset_train_final.pickle\", 'rb') as f:\n",
    "    dataset_train_final = pickle.load(f)\n",
    "\n",
    "with open(\"./label_train.pickle\", 'rb') as f:\n",
    "    label_train = pickle.load(f)\n",
    "\n",
    "with open(\"./dataset_test_final.pickle\", 'rb') as f:\n",
    "    dataset_test_final = pickle.load(f)\n",
    "\n",
    "with open(\"./label_test.pickle\", 'rb') as f:\n",
    "    label_test = pickle.load(f)\n",
    "\n",
    "with open(\"./dataset_cv_final.pickle\", 'rb') as f:\n",
    "    dataset_cv_final = pickle.load(f)\n",
    "\n",
    "with open(\"./label_cv.pickle\", 'rb') as f:\n",
    "    label_cv = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, after setting up dataset, I need to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset_train_final,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the Logistic_regression model object, for further,so we don't need to train\n",
    "# again and again.\n",
    "\n",
    "with open(\"./Logistic_regression_fit.pickle\",'wb') as f:\n",
    "    pickle.dump(model,f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./Logistic_regression_fit.pickle\", 'rb') as f:\n",
    "    model_from_pickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicc = model_from_pickle.predict(dataset_cv_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946666666667\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(predicc,label_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yay!!!, got a accuracy of about 94.7%. Logistic Regression works like Charm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Let's try with SVM classifer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 120000)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "classifier = SVC()\n",
    "print X_shuf.shape\n",
    "print Y_shuf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_shuf,Y_shuf)\n",
    "classifier.fit(dataset_train_final,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicc = classifier.predict(dataset_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(predicc,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  So i got a accuracy of 30% with SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression is used to ascertain the probability of an event. \n",
    "# And this event is captured in binary format, i.e. 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So it comes out SVM's are not good if features are quite large(120000) and Logistic Regression outperforms it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
