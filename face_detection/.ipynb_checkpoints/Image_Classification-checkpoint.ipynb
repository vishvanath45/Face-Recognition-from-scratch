{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precog Project\n",
    "## Image Recognition Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  I have a folder in which named clean_dataset, which has 3 sub folder named namo, ak and people \n",
    "#  containing 50 images in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Basic Idea about problem is that is looks like typical Classification Problem, \n",
    "# Where classes can be namely NAMO, AK, People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "# import dlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "import pymongo\n",
    "from bson import Binary\n",
    "import bson\n",
    "from bson.binary import Binary as BsonBinary\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connection = MongoClient()\n",
    "db = connection['mongo_database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now want to convert the entire dataset into a 3D array\n",
    "# (image index, x, y), with mean = 0  and s.d. = 0.5\n",
    "#  give 3 diff folderpaths namo, ak, people\n",
    "img_wd = 200 # resized Images\n",
    "img_ht = 200 # resized Images\n",
    "\n",
    "def load_pictures(folderpath):\n",
    "    '''Loading for a single image'''\n",
    "    img_files = os.listdir(folderpath)\n",
    "    tot_imgs = len(img_files)\n",
    "\n",
    "#     creating dataset for images of each class, dataset will be of size [totalimages * (img_wd*img_ht*3 =(600))]\n",
    "    dataset = np.ndarray(shape = (tot_imgs, img_wd*img_ht*3),\n",
    "                         dtype = np.float32)\n",
    "    img_index = 0\n",
    "    for img in img_files:\n",
    "        img_file_path = os.path.join(folderpath, img)\n",
    "        try:\n",
    "            img_data = (ndimage.imread(img_file_path).astype(float) - 255.0/2)/255.0\n",
    "#              We did the above to set the range of the image data to \n",
    "#             -0.5 to 0.5 (s.d.)\n",
    "\n",
    "#             reshaping a (200,200,3) matrix to (12000), flattening\n",
    "            img_data = img_data.reshape(img_wd*img_ht*3)\n",
    "    \n",
    "#     adding flattened img_data to dataset\n",
    "            dataset[img_index, :] = img_data\n",
    "            img_index = img_index + 1\n",
    "        except IOError as e:\n",
    "            print('file read error, skipping thsi file')\n",
    "                \n",
    "    print('Full dataset shape', dataset.shape)\n",
    "    print('Dataset Mean', np.mean(dataset))\n",
    "    print('Dataset SD', np.std(dataset))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharma/.local/lib/python2.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', -0.06528978)\n",
      "('Dataset SD', 0.24625796)\n",
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', 0.0063913618)\n",
      "('Dataset SD', 0.22900325)\n",
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', -0.030012265)\n",
      "('Dataset SD', 0.2670311)\n"
     ]
    }
   ],
   "source": [
    "data_set_ak = load_pictures(\"./clean_dataset/ak\")\n",
    "data_set_namo = load_pictures(\"./clean_dataset/namo\")\n",
    "data_set_people = load_pictures(\"./clean_dataset/people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now i have created Flattened Image data set for each three \n",
    "# of categories\n",
    "\n",
    "#  one such db can be \n",
    "# [ R,G,B,R,G,B,R,G,B,R,G,B,R,G,B,R,G,B ..... 200*200  ] pixals\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# for all images for a given person\n",
    "# so now size of a given db is (400*20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now I have to divide dataset in TRAIN, TEST, CROSS_VALIDATION\n",
    "# total 400 images of each\n",
    "# 50% --> TRAIN (300 imgs in each class)\n",
    "# 25% -->TEST (50 imgs '' )\n",
    "# 25% -->CV (50 imgs '')\n",
    "# CV = Cross Validation Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_train_namo = data_set_namo[0:300]\n",
    "dataset_test_namo = data_set_namo[300:350]\n",
    "dataset_cv_namo = data_set_namo[350:400]\n",
    "\n",
    "dataset_train_ak = data_set_ak[0:300]\n",
    "dataset_test_ak = data_set_ak[300:350]\n",
    "dataset_cv_ak = data_set_ak[350:400]\n",
    "\n",
    "dataset_train_people = data_set_people[0:300]\n",
    "dataset_test_people = data_set_people[300:350]\n",
    "dataset_cv_people = data_set_people[350:400]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 120000)\n",
      "(150, 120000)\n",
      "(150, 120000)\n"
     ]
    }
   ],
   "source": [
    "dataset_train_final = np.concatenate((dataset_train_namo,dataset_train_ak))\n",
    "dataset_train_final = np.concatenate((dataset_train_final,dataset_train_people))\n",
    "print dataset_train_final.shape\n",
    "\n",
    "dataset_test_final = np.concatenate((dataset_test_namo,dataset_test_ak))\n",
    "dataset_test_final = np.concatenate((dataset_test_final,dataset_test_people))\n",
    "print dataset_test_final.shape\n",
    "\n",
    "dataset_cv_final = np.concatenate((dataset_cv_namo,dataset_cv_ak))\n",
    "dataset_cv_final = np.concatenate((dataset_cv_final,dataset_cv_people))\n",
    "print dataset_test_final.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  next task to create a label for each dataset \n",
    "# if 1 represents modi\n",
    "# it should be like\n",
    "# [1]\n",
    "# [1]\n",
    "# [1]\n",
    "# [1]\n",
    "\n",
    "# CONVENTION -\n",
    "# 1 --> NAMO\n",
    "# 2 --> AK\n",
    "# 3 --> PEOPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# total row in train data set = 900\n",
    "\n",
    "label_train = np.ndarray(900, dtype=np.int32)\n",
    "label_train[0:300]=1 # for namo \n",
    "label_train[300:600]=2 # for ak\n",
    "label_train[600:900]=3 # for people\n",
    "print label_train.size\n",
    "\n",
    "label_test = np.ndarray(150, dtype=np.int32)\n",
    "label_test[0:50]=1 # for namo \n",
    "label_test[50:100]=2 # for ak\n",
    "label_test[100:150]=3 # for people\n",
    "print label_test.size\n",
    "\n",
    "\n",
    "label_cv = np.ndarray(150, dtype=np.int32)\n",
    "label_cv[0:50]=1 # for namo \n",
    "label_cv[50:100]=2 # for ak\n",
    "label_cv[100:150]=3 # for people\n",
    "print label_cv.size\n",
    "\n",
    "#     just an other way to label, I was trying\n",
    "# label_array_namo = np.full((400,1),1)\n",
    "# label_array_ak = np.full((400,1),2)\n",
    "# label_array_people = np.full((400,1),3)\n",
    "# # print label_array_ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# both dataset and labels are arranged w.r.t. variable perm \n",
    "\n",
    "def randomize(dataset,labels,size):\n",
    "    perm = np.random.permutation(size)\n",
    "    shuffled_dataset = dataset[perm,:]\n",
    "    shuffled_labels = labels[perm]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 120000) (900,)\n",
      "(150, 120000) (150,)\n",
      "(150, 120000) (150,)\n",
      "[1 3 3 3 3 3 2 2 1 2 1 1 3 3 1 3 3 2 3 2 3 3 3 3 2 2 3 1 1 2 2 1 1 1 2 2 1\n",
      " 1 2 1 2 2 1 1 2 2 1 2 3 2 3 3 1 3 2 1 1 3 1 3 3 2 2 1 3 2 3 2 1 3 1 3 3 2\n",
      " 2 1 1 2 3 3 1 3 2 2 1 2 1 1 2 2 3 1 1 3 2 1 2 1 2 3 1 1 2 2 3 3 3 3 1 1 3\n",
      " 2 1 2 2 2 1 2 1 3 2 3 3 1 2 1 3 1 1 1 3 3 2 1 2 2 3 1 3 3 2 1 3 3 2 2 1 1\n",
      " 2 3]\n"
     ]
    }
   ],
   "source": [
    "dataset_train_final, label_train = randomize(dataset_train_final,label_train,900)\n",
    "dataset_test_final, label_test = randomize(dataset_test_final,label_test,150)\n",
    "dataset_cv_final, label_cv = randomize(dataset_cv_final,label_cv,150)\n",
    "\n",
    "print dataset_train_final.shape,label_train.shape\n",
    "print dataset_test_final.shape, label_test.shape\n",
    "print dataset_cv_final.shape, label_cv.shape\n",
    "# A sample printout, to be sure of ramdomised data\n",
    "print label_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharma/.local/lib/python2.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    },
    {
     "ename": "DocumentTooLarge",
     "evalue": "BSON document too large (1484657743 bytes) - the connected server supports BSON document sizes up to 16793598 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDocumentTooLarge\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-b10dfbfd532a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# db.insert({'dataset_train_final': Binary(pickle.dumps(dataset_train_final))})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_train_final'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dataset_train_final'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'label_train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_test_final'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dataset_test_final'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/collection.pyc\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, doc_or_docs, manipulate, check_keys, continue_on_error, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/collection.pyc\u001b[0m in \u001b[0;36m_insert\u001b[0;34m(self, sock_info, docs, ordered, check_keys, manipulate, write_concern, op_id, bypass_doc_val)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/collection.pyc\u001b[0m in \u001b[0;36m_insert_one\u001b[0;34m(self, sock_info, doc, ordered, check_keys, manipulate, write_concern, op_id, bypass_doc_val)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/pool.pyc\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/pool.pyc\u001b[0m in \u001b[0;36m_raise_connection_failure\u001b[0;34m(self, error)\u001b[0m\n",
      "\u001b[0;31mDocumentTooLarge\u001b[0m: BSON document too large (1484657743 bytes) - the connected server supports BSON document sizes up to 16793598 bytes."
     ]
    }
   ],
   "source": [
    "# saving files to mongoDB\n",
    "#  THE ERROR I COULD NOT SAVE FILES IN mongoDB\n",
    "#  mongoDB only allows file upto size 16MB\n",
    "# this works, YAY!! \\/,\n",
    "# db.insert({'dataset_train_final': Binary(pickle.dumps(dataset_train_final))})\n",
    "\n",
    "db['dataset_train_final'].insert({'dataset_train_final': Binary(pickle.dumps(dataset_train_final))})\n",
    "db['label_train'].insert({'label_train': Binary(pickle.dumps(label_train))})\n",
    "db['dataset_test_final'].insert({'dataset_test_final': Binary(pickle.dumps(dataset_test_final))})\n",
    "db['label_test'].insert({'label_test': Binary(pickle.dumps(label_test))})\n",
    "db['dataset_cv_final'].insert({'dataset_cv_final': Binary(pickle.dumps(dataset_cv_final))})\n",
    "db['label_cv'].insert({'label_cv': Binary(pickle.dumps(label_cv))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving files to .pickle\n",
    "\n",
    "with open(\"./dataset_train_final.pickle\",'wb') as f:\n",
    "    pickle.dump(dataset_train_final,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./label_train.pickle\",'wb') as f:\n",
    "    pickle.dump(label_train,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./dataset_test_final.pickle\",'wb') as f:\n",
    "    pickle.dump(dataset_test_final,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./label_test.pickle\",'wb') as f:\n",
    "    pickle.dump(label_test,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./dataset_cv_final.pickle\",'wb') as f:\n",
    "    pickle.dump(dataset_cv_final,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./label_cv.pickle\",'wb') as f:\n",
    "    pickle.dump(label_cv,f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for fetching data from mongoDB \n",
    "#  Could not use it, beacause unable to save in mongoDB\n",
    "# dataset_train_final = [np.array(x['dataset_train_final']) for x in db['dataset_train_final'].find()]\n",
    "# label_train = [np.array(x['label_train']) for x in db['label_train'].find()]\n",
    "# dataset_test_final = [np.array(x['dataset_test_final']) for x in db['dataset_test_final'].find()]\n",
    "# label_test = [np.array(x['label_test']) for x in db['label_test'].find()]\n",
    "# dataset_cv_final = [np.array(x['dataset_cv_final']) for x in db['dataset_cv_final'].find()]\n",
    "# label_cv = [np.array(x['label_cv']) for x in db['label_cv'].find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  for fetching data from .pickle files\n",
    "\n",
    "with open(\"./dataset_train_final.pickle\", 'rb') as f:\n",
    "    dataset_train_final = pickle.load(f)\n",
    "\n",
    "with open(\"./label_train.pickle\", 'rb') as f:\n",
    "    label_train = pickle.load(f)\n",
    "\n",
    "with open(\"./dataset_test_final.pickle\", 'rb') as f:\n",
    "    dataset_test_final = pickle.load(f)\n",
    "\n",
    "with open(\"./label_test.pickle\", 'rb') as f:\n",
    "    label_test = pickle.load(f)\n",
    "\n",
    "with open(\"./dataset_cv_final.pickle\", 'rb') as f:\n",
    "    dataset_cv_final = pickle.load(f)\n",
    "\n",
    "with open(\"./label_cv.pickle\", 'rb') as f:\n",
    "    label_cv = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Begins     \\m/    (0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, after setting up dataset, I need to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset_train_final,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving the Logistic_regression model object, for further,so we don't need to train\n",
    "# again and again.\n",
    "\n",
    "with open(\"./Logistic_regression_fit.pickle\",'wb') as f:\n",
    "    pickle.dump(model,f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./Logistic_regression_fit.pickle\", 'rb') as f:\n",
    "    model_from_pickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicc = model_from_pickle.predict(dataset_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946666666667\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(predicc,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  running it on cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicc = model_from_pickle.predict(dataset_cv_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.953333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(predicc,label_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yay!!!, got a accuracy of about 95.3%., :D Logistic Regression works like Charm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Let's try with SVM classifer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 120000)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "classifier = SVC()\n",
    "print X_shuf.shape\n",
    "print Y_shuf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_shuf,Y_shuf)\n",
    "classifier.fit(dataset_train_final,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicc = classifier.predict(dataset_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(predicc,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  So i got a accuracy of 30% with SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression is used to ascertain the probability of an event. \n",
    "# And this event is captured in binary format, i.e. 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So it comes out SVM's are not good if features are quite large(120000) and Logistic Regression outperforms it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
