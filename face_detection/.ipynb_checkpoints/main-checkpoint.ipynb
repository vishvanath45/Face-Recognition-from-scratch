{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precog Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  I have a folder in which named TEST_dataset, which has 3 sub folder named namo, ak , and people \n",
    "#  containing 50 images in each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "# import dlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "np.random.seed(133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now want to convert the entire dataset into a 3D array\n",
    "# (image index, x, y), with mean = 0  and s.d. = 0.5\n",
    "#  give 3 diff folderpaths namo, ak, people\n",
    "img_wd = 200 #it is known\n",
    "img_ht = 200 #it is known\n",
    "\n",
    "def load_pictures(folderpath):\n",
    "    '''Loading for a single image'''\n",
    "    img_files = os.listdir(folderpath)\n",
    "    tot_imgs = len(img_files)\n",
    "    \n",
    "#     print img_files[1]\n",
    "#     full_path = os.path.join(folderpath,img_files[1])\n",
    "#     img_data = (ndimage.imread(full_path).astype(float) - 255.0/2)/255.0\n",
    "#     for converting (200,200,3)\n",
    "#     img_data = img_data[:,:,0]\n",
    "#     print img_data.shape\n",
    "#     print img_data\n",
    "#     new_arr = img_data.reshape(120000)\n",
    "#     print new_arr\n",
    "    \n",
    "    dataset = np.ndarray(shape = (tot_imgs, img_wd*img_ht*3),\n",
    "                         dtype = np.float32)\n",
    "    img_index = 0\n",
    "    for img in img_files:\n",
    "        img_file_path = os.path.join(folderpath, img)\n",
    "        try:\n",
    "            img_data = (ndimage.imread(img_file_path).astype(float) - 255.0/2)/255.0\n",
    "#             # We did the above to set the range of the image data to \n",
    "#             # -0.5 to 0.5 (s.d.)\n",
    "            img_data = img_data.reshape(img_wd*img_ht*3)\n",
    "            dataset[img_index, :] = img_data\n",
    "            img_index = img_index + 1\n",
    "        except IOError as e:\n",
    "            print('file read error, skipping thsi file')\n",
    "                \n",
    "    print('Full dataset shape', dataset.shape)\n",
    "    print('Dataset Mean', np.mean(dataset))\n",
    "    print('Dataset SD', np.std(dataset))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', -0.065289773)\n",
      "('Dataset SD', 0.24625809)\n",
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', 0.0063913576)\n",
      "('Dataset SD', 0.22900333)\n",
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', -0.030012256)\n",
      "('Dataset SD', 0.26703089)\n"
     ]
    }
   ],
   "source": [
    "data_set_ak = load_pictures(\"./TEST_dataset/ak\")\n",
    "data_set_namo = load_pictures(\"./TEST_dataset/namo\")\n",
    "data_set_people = load_pictures(\"./TEST_dataset/people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i have created Flattened Image data set for each three \n",
    "# of categories\n",
    "\n",
    "#  one such db can be \n",
    "# [ R,G,B,R,G,B,R,G,B,R,G,B,R,G,B,R,G,B ..... 200*200  ] pixals\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# for all images for a given person\n",
    "# so now size of a given db is (400*20000)\n",
    "#  next task to create a label for each dataset \n",
    "# if 1 represents modi\n",
    "# it should be like\n",
    "# [1]\n",
    "# [1]\n",
    "# [1]\n",
    "# [1]\n",
    "# CONVENTION\n",
    "# 1 --> NAMO\n",
    "# 2 --> AK\n",
    "# 3 --> PEOPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I have to divide dataset in TRAIN, TEST, CROSS_VALIDATION\n",
    "# total 400 images of each\n",
    "# 50% --> TRAIN (300 imgs in each class)\n",
    "# 25% -->TEST (50 imgs '' )\n",
    "# 25% -->CV (50 imgs '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_namo = data_set_namo[0:300]\n",
    "dataset_test_namo = data_set_namo[300:350]\n",
    "dataset_cv_namo = data_set_namo[350:400]\n",
    "\n",
    "dataset_train_ak = data_set_ak[0:300]\n",
    "dataset_test_ak = data_set_ak[300:350]\n",
    "dataset_cv_ak = data_set_ak[350:400]\n",
    "\n",
    "dataset_train_people = data_set_people[0:300]\n",
    "dataset_test_people = data_set_people[300:350]\n",
    "dataset_cv_people = data_set_people[350:400]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 120000)\n",
      "(150, 120000)\n",
      "(150, 120000)\n"
     ]
    }
   ],
   "source": [
    "dataset_train_final = np.concatenate((dataset_train_namo,dataset_train_ak))\n",
    "dataset_train_final = np.concatenate((dataset_train_final,dataset_train_people))\n",
    "print dataset_train_final.shape\n",
    "\n",
    "dataset_test_final = np.concatenate((dataset_test_namo,dataset_test_ak))\n",
    "dataset_test_final = np.concatenate((dataset_test_final,dataset_test_people))\n",
    "print dataset_test_final.shape\n",
    "\n",
    "dataset_cv_final = np.concatenate((dataset_cv_namo,dataset_cv_ak))\n",
    "dataset_cv_final = np.concatenate((dataset_cv_final,dataset_cv_people))\n",
    "print dataset_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# total row in train data set = 900\n",
    "\n",
    "label_train = np.ndarray(900, dtype=np.int32)\n",
    "label_train[0:300]=1 # for namo \n",
    "label_train[300:600]=2 # for ak\n",
    "label_train[600:900]=3 # for people\n",
    "print label_train.size\n",
    "\n",
    "label_test = np.ndarray(150, dtype=np.int32)\n",
    "label_test[0:50]=1 # for namo \n",
    "label_test[50:100]=2 # for ak\n",
    "label_test[100:150]=3 # for people\n",
    "print label_test.size\n",
    "\n",
    "\n",
    "label_cv = np.ndarray(150, dtype=np.int32)\n",
    "label_cv[0:50]=1 # for namo \n",
    "label_cv[50:100]=2 # for ak\n",
    "label_cv[100:150]=3 # for people\n",
    "print label_cv.size\n",
    "#     just an other way to label, I was trying\n",
    "# label_array_namo = np.full((400,1),1)\n",
    "# label_array_ak = np.full((400,1),2)\n",
    "# label_array_people = np.full((400,1),3)\n",
    "# # print label_array_ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomizing the database w.r.t. to labels\n",
    "def randomize(dataset,labels,size):\n",
    "    perm = np.random.permutation(size)\n",
    "    shuffled_dataset = dataset[perm,:]\n",
    "    shuffled_labels = labels[perm]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 120000) (900,)\n",
      "(150, 120000) (150,)\n",
      "(150, 120000) (150,)\n"
     ]
    }
   ],
   "source": [
    "dataset_train_final, label_train = randomize(dataset_train_final,label_train,900)\n",
    "dataset_test_final, label_test = randomize(dataset_test_final,label_test,150)\n",
    "dataset_cv_final, label_cv = randomize(dataset_cv_final,label_cv,150)\n",
    "\n",
    "print dataset_train_final.shape,label_train.shape\n",
    "print dataset_test_final.shape, label_test.shape\n",
    "print dataset_cv_final.shape, label_cv.shape\n",
    "# print label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files to pickle\n",
    "\n",
    "with open(\"./dataset_train_final.pickle\",'wb') as f:\n",
    "    pickle.dump(dataset_train_final,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./label_train.pickle\",'wb') as f:\n",
    "    pickle.dump(label_train,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./dataset_test_final.pickle\",'wb') as f:\n",
    "    pickle.dump(dataset_test_final,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./label_test.pickle\",'wb') as f:\n",
    "    pickle.dump(label_test,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./dataset_cv_final.pickle\",'wb') as f:\n",
    "    pickle.dump(dataset_cv_final,f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./label_cv.pickle\",'wb') as f:\n",
    "    pickle.dump(label_cv,f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for fetching data from .pickle files\n",
    "# with open(\"./dataset_train_final.pickle\", 'rb') as f:\n",
    "#     dataset_train_final = pickle.load(f)\n",
    "\n",
    "# with open(\"./label_train.pickle\", 'rb') as f:\n",
    "#     label_train = pickle.load(f)\n",
    "\n",
    "# with open(\"./dataset_test_final.pickle\", 'rb') as f:\n",
    "#     dataset_test_final = pickle.load(f)\n",
    "\n",
    "# with open(\"./label_test.pickle\", 'rb') as f:\n",
    "#     label_test = pickle.load(f)\n",
    "\n",
    "# with open(\"./dataset_cv_final.pickle\", 'rb') as f:\n",
    "#     dataset_cv_final = pickle.load(f)\n",
    "\n",
    "# with open(\"./label_cv.pickle\", 'rb') as f:\n",
    "#     label_cv = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108000000\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "# There was a known issue in LR, so currently trying to avoid it\n",
    "model = LogisticRegression()\n",
    "print dataset_train_final.size\n",
    "print label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset_train_final,label_train)\n",
    "predict_LR = model.predict(dataset_test_final)\n",
    "print(accuracy_score(predict_LR,label_test))\n",
    "# accuracy 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "gnb = tree.DecisionTreeClassifier()\n",
    "gnb.fit(dataset_train_final,label_train)\n",
    "predict_DT = gnb.predict(dataset_test_final)\n",
    "print(accuracy_score(predict_DT,label_test))\n",
    "# accuracy 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = SVC()\n",
    "# classifier.fit(dataset_train_final,label_train)\n",
    "# predicc = classifier.predict(dataset_test_final)\n",
    "# print(accuracy_score(predicc,label_test))\n",
    "# This gave me 0.3 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(x):\n",
    "#     #using formulae = (x-x(min))/(x(max)-x(min))\n",
    "#     x_min = np.min(x)\n",
    "#     x_max = np.max(x)\n",
    "#     x_final_normalised = list()\n",
    "# #     print \"xmin \", x_min\n",
    "# #     print \"xmax \", x_max\n",
    "    \n",
    "#     for i in x:\n",
    "# #         print i\n",
    "#         x_final_normalised.append((i-x_min)/(x_max-x_min))\n",
    "        \n",
    "#     return np.array(x_final_normalised)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalised_array = normalize(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode(x):\n",
    "#     y = np.zeros(3)\n",
    "#     np.put(y,x,1)\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot_encode(x):\n",
    "#     one_hot = list()\n",
    "    \n",
    "#     for i in x:\n",
    "#         one_hot.append(encode(i))\n",
    "#     return np.array(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# something to be done here, preprocess training,\n",
    "# valid, and testing set\n",
    "# preprossing includes, normalising, one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave it above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
