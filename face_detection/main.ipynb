{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precog Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  I have a folder in which named TEST_dataset, which has 3 sub folder named namo, ak , and people \n",
    "#  containing 50 images in each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "# import dlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now want to convert the entire dataset into a 3D array\n",
    "# (image index, x, y), with mean = 0  and s.d. = 0.5\n",
    "#  give 3 diff folderpaths namo, ak, people\n",
    "img_wd = 200 #it is known\n",
    "img_ht = 200 #it is known\n",
    "\n",
    "def load_pictures(folderpath):\n",
    "    '''Loading for a single image'''\n",
    "    img_files = os.listdir(folderpath)\n",
    "    tot_imgs = len(img_files)\n",
    "    \n",
    "#     print img_files[1]\n",
    "#     full_path = os.path.join(folderpath,img_files[1])\n",
    "#     img_data = (ndimage.imread(full_path).astype(float) - 255.0/2)/255.0\n",
    "#     for converting (200,200,3)\n",
    "#     img_data = img_data[:,:,0]\n",
    "#     print img_data.shape\n",
    "#     print img_data\n",
    "#     new_arr = img_data.reshape(120000)\n",
    "#     print new_arr\n",
    "    \n",
    "    dataset = np.ndarray(shape = (tot_imgs, img_wd*img_ht*3),\n",
    "                         dtype = np.float32)\n",
    "    img_index = 0\n",
    "    for img in img_files:\n",
    "        img_file_path = os.path.join(folderpath, img)\n",
    "        try:\n",
    "            img_data = (ndimage.imread(img_file_path).astype(float) - 255.0/2)/255.0\n",
    "#             # We did the above to set the range of the image data to \n",
    "#             # -0.5 to 0.5 (s.d.)\n",
    "            img_data = img_data.reshape(img_wd*img_ht*3)\n",
    "            dataset[img_index, :] = img_data\n",
    "            img_index = img_index + 1\n",
    "        except IOError as e:\n",
    "            print('file read error, skipping thsi file')\n",
    "                \n",
    "    print('Full dataset shape', dataset.shape)\n",
    "    print('Dataset Mean', np.mean(dataset))\n",
    "    print('Dataset SD', np.std(dataset))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharma/.local/lib/python2.7/site-packages/ipykernel_launcher.py:28: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', -0.065289788)\n",
      "('Dataset SD', 0.2462582)\n",
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', 0.0063913679)\n",
      "('Dataset SD', 0.22900349)\n",
      "('Full dataset shape', (400, 120000))\n",
      "('Dataset Mean', -0.030012263)\n",
      "('Dataset SD', 0.26703113)\n"
     ]
    }
   ],
   "source": [
    "data_set_ak = load_pictures(\"./TEST_dataset/ak\")\n",
    "data_set_namo = load_pictures(\"./TEST_dataset/namo\")\n",
    "data_set_people = load_pictures(\"./TEST_dataset/people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now i have created Flattened Image data set for each three \n",
    "# of categories\n",
    "\n",
    "#  one such db can be \n",
    "# [ R,G,B,R,G,B,R,G,B,R,G,B,R,G,B,R,G,B ..... 200*200  ] pixals\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# for all images for a given person\n",
    "# so now size of a given db is (400*20000)\n",
    "#  next task to create a label for each dataset \n",
    "# if 1 represents modi\n",
    "# it should be like\n",
    "# [1]\n",
    "# [1]\n",
    "# [1]\n",
    "# [1]\n",
    "# CONVENTION\n",
    "# 1 --> NAMO\n",
    "# 2 --> AK\n",
    "# 3 --> PEOPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now I have to divide dataset in TRAIN, TEST, CROSS_VALIDATION\n",
    "# total 400 images of each\n",
    "# 50% --> TRAIN (300 imgs in each class)\n",
    "# 25% -->TEST (50 imgs '' )\n",
    "# 25% -->CV (50 imgs '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28823531  0.08431373  0.02941176 ..., -0.5        -0.49607843\n",
      "  -0.44901961]\n",
      " [ 0.06470589 -0.01372549 -0.05686275 ...,  0.10784314  0.09607843\n",
      "   0.06862745]\n",
      " [-0.11960784 -0.12352941 -0.14313726 ...,  0.15490197  0.07254902\n",
      "   0.06078431]\n",
      " ..., \n",
      " [-0.21372549 -0.32745099 -0.24901961 ..., -0.5        -0.5        -0.5       ]\n",
      " [ 0.26862746  0.08431373 -0.01764706 ..., -0.10392157 -0.05686275\n",
      "   0.13921569]\n",
      " [ 0.40196079  0.34705883  0.29607844 ..., -0.20980392 -0.2764706\n",
      "  -0.34705883]]\n"
     ]
    }
   ],
   "source": [
    "dataset_train_namo = data_set_namo[0:300]\n",
    "dataset_test_namo = data_set_namo[300:350]\n",
    "dataset_cv_namo = data_set_namo[350:400]\n",
    "\n",
    "dataset_train_ak = data_set_ak[0:300]\n",
    "dataset_test_ak = data_set_ak[300:350]\n",
    "dataset_cv_ak = data_set_ak[350:400]\n",
    "\n",
    "dataset_train_people = data_set_people[0:300]\n",
    "dataset_test_people = data_set_people[300:350]\n",
    "dataset_cv_people = data_set_people[350:400]\n",
    "print dataset_train_namo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28823531  0.08431373  0.02941176 ..., -0.5        -0.49607843\n",
      "  -0.44901961]\n",
      " [ 0.06470589 -0.01372549 -0.05686275 ...,  0.10784314  0.09607843\n",
      "   0.06862745]\n",
      " [-0.11960784 -0.12352941 -0.14313726 ...,  0.15490197  0.07254902\n",
      "   0.06078431]\n",
      " ..., \n",
      " [-0.23333333 -0.27254903 -0.31960785 ...,  0.28431374  0.31568629\n",
      "   0.32352942]\n",
      " [ 0.10392157  0.0254902  -0.10392157 ...,  0.04901961 -0.02941176\n",
      "  -0.12352941]\n",
      " [-0.23333333 -0.28039217 -0.33529413 ...,  0.17450981  0.00588235\n",
      "  -0.06862745]]\n"
     ]
    }
   ],
   "source": [
    "dataset_train_final = np.concatenate((dataset_train_namo,dataset_train_ak))\n",
    "dataset_train_final = np.concatenate((dataset_train_final,dataset_train_people))\n",
    "print dataset_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# label_array_namo = np.full((400,1),1)\n",
    "# label_array_ak = np.full((400,1),2)\n",
    "# label_array_people = np.full((400,1),3)\n",
    "# # print label_array_ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  this is if i want to merge label in dataset individaul\n",
    "# final_dataset_namo = np.append(data_set_namo,label_array_namo,axis=1)\n",
    "# final_dataset_ak = np.append(data_set_ak,label_array_ak,axis=1)\n",
    "# final_dataset_people = np.append(data_set_people,label_array_people,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total image in each image is = 400\n",
    "label_array = np.ndarray(400*3, dtype=np.int32)\n",
    "# total image in each image is = 50\n",
    "label_array[0:400] = 1\n",
    "label_array[400:800] = 2\n",
    "label_array[800:1200] = 3\n",
    "# label_array_namo.fill(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def normalize(x):\n",
    "#     #using formulae = (x-x(min))/(x(max)-x(min))\n",
    "#     x_min = np.min(x)\n",
    "#     x_max = np.max(x)\n",
    "#     x_final_normalised = list()\n",
    "# #     print \"xmin \", x_min\n",
    "# #     print \"xmax \", x_max\n",
    "    \n",
    "#     for i in x:\n",
    "# #         print i\n",
    "#         x_final_normalised.append((i-x_min)/(x_max-x_min))\n",
    "        \n",
    "#     return np.array(x_final_normalised)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalised_array = normalize(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def encode(x):\n",
    "#     y = np.zeros(3)\n",
    "#     np.put(y,x,1)\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def one_hot_encode(x):\n",
    "#     one_hot = list()\n",
    "    \n",
    "#     for i in x:\n",
    "#         one_hot.append(encode(i))\n",
    "#     return np.array(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# something to be done here, preprocess training,\n",
    "# valid, and testing set\n",
    "# preprossing includes, normalising, one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# leave it above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
